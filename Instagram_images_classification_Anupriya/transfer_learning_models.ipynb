{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCCjUPYXId2g"
      },
      "source": [
        "### Pre-requisite - Download Datasets\n",
        "Run all the cells below after downloading the required kaggle.json file (follow [these steps](https://www.kaggle.com/discussions/general/156610))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnRTm1as5bAv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEPBdBI35gpF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TukCYQd75ygu"
      },
      "outputs": [],
      "source": [
        "!ls -lha kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIX8VjjI50IO"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWJYqxgs52O5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e50YZGKS54yB"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzWtOaF6561h"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv6iw7z558VX"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d anupriyakkumari/instagram-5-classes-dataset-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tddLrD895_PB"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d anupriyakkumari/instagram-5-classes-dataset-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQAYhvU45_qg"
      },
      "outputs": [],
      "source": [
        "!unzip instagram-5-classes-dataset-1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMu7FWbJ6BoS"
      },
      "outputs": [],
      "source": [
        "!unzip instagram-5-classes-dataset-2.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS96Kk9MOpO8"
      },
      "source": [
        "* Note - rename the folders for consistency.\n",
        "We renamed them to - Instagram_Dataset_1 and Instagram_Dataset_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScycCiBZCcdI"
      },
      "source": [
        "#1. Transfer Learning Model - Xception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dz4L2vjGNRq"
      },
      "source": [
        "### 1.1 Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g86rblgTHpmM"
      },
      "source": [
        "The following approach uses transfer learning on base model Xception. We need to run each cell one by one. Importing these libraries is first step.\n",
        "Then we need our dataset divided into train, validation and test folders (each with subfolders of 5 classes) and uploaded on colab (either by mounting drive or downloading directly from kaggle in the colab notebook). The paths of each dataset directory can be changed as needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUVwnrqpc_wU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlz6mR4yU2A2"
      },
      "source": [
        "Here, we are uploading our files from each of the folders and subfolders, setting the parameters as required for the model and printing the class names, creating a dictionary to access class names, and printing number of batches in each folder - train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxXDghoiRLXj"
      },
      "outputs": [],
      "source": [
        "dir_train = \"/content/Instagram_Dataset_1/classes\"\n",
        "dir_new=\"/content/Instagram_Dataset_2/classes\"\n",
        "#using smaller dataset for now\n",
        "train_ds=tf.keras.utils.image_dataset_from_directory(\n",
        "    dir_train,\n",
        "    color_mode=\"rgb\",\n",
        "    image_size=(150, 150),\n",
        "    validation_split=0.1,\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    seed=2,\n",
        "    batch_size=64\n",
        "    )\n",
        "\n",
        "validation_ds=tf.keras.utils.image_dataset_from_directory(\n",
        "    dir_train,\n",
        "    color_mode=\"rgb\",\n",
        "    image_size=(150, 150),\n",
        "    validation_split=0.1,\n",
        "    subset=\"validation\",\n",
        "    shuffle=True,\n",
        "    seed=2,\n",
        "    batch_size=64\n",
        "    )\n",
        "directory_test1=\"/content/Instagram_Dataset_1/test\"\n",
        "directory_test2=\"/content/Instagram_Dataset_2/test\"\n",
        "test_ds=tf.keras.utils.image_dataset_from_directory(\n",
        "    directory_test1,\n",
        "    color_mode=\"rgb\",\n",
        "    image_size=(150, 150),\n",
        "    shuffle=True,\n",
        "    seed=2,\n",
        "    batch_size=64\n",
        "    )\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)\n",
        "#class_names_dic1={0:'animals',1:'beauty',2:'food',3:'memes',4:'travel'}\n",
        "class_names_dic2={0:'beauty',1:'food',2:'memes',3:'pets',4:'travel'}\n",
        "\n",
        "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
        "print(\n",
        "    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n",
        ")\n",
        "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tnrF5s-VQBX"
      },
      "source": [
        "Now, by running this cell, we get the shape of the image and label as it gets uploaded using tf.keras.utils.image_dataset_from_directory first as batches and in those batches are images (float32 tensor) and labels (int32 tensor). We created batches of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7Oz1Dq7qUR_"
      },
      "outputs": [],
      "source": [
        "for image, label in train_ds.take(1):\n",
        "  #print element in the tuple of train_ds which has 2 tuples inside indicating (batchsize,height,width,channels) and (batchsize,)\n",
        "    print(image.shape, label.shape)\n",
        "    for image, label in zip(image, label):\n",
        "      #each batch further has 64 images each in it stored in zip(image,label)\n",
        "        print(image.shape, label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxEKVzM4Vzj_"
      },
      "source": [
        "By running the cell below, we plot the 10 images from one batch randomly in a 2x5 plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ_blfQUyoLl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "class_names = list(train_ds.class_names)\n",
        "# Create a figure with 2 rows and 5 columns of subplots\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10,5))\n",
        "\n",
        "# Flatten the axes array to simplify indexing\n",
        "axes = axes.flatten()\n",
        "for images, labels in train_ds:\n",
        "  # Plot each image on a subplot\n",
        "  for i in range(len(images)):\n",
        "    axes[i].imshow(images[i].numpy().astype('uint8'))\n",
        "    axes[i].set_title(class_names_dic2[(labels[i].numpy())])\n",
        "    axes[i].axis('off')\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg6arY76WF3h"
      },
      "source": [
        "Here we are creating a  data augmentation layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A43kM-6aric"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1), layers.RandomContrast(factor=0.2),\n",
        "]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZDfwJsvYZ_X"
      },
      "source": [
        "Here we are visualizing the first augmented image from the first batch and showing the variation produced on a 9x9 plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffdhSr55asXg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for images, labels in train_ds.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = images[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(\n",
        "            tf.expand_dims(first_image, 0), training=True\n",
        "        )\n",
        "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
        "        plt.title(int(labels[0]))\n",
        "        plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-ApCpbGZSn5"
      },
      "source": [
        "Now we get a base model, Xception (trained on Imagenet) and proceed to freeze it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_1rryIgaxYE"
      },
      "outputs": [],
      "source": [
        "base_model = keras.applications.Xception(\n",
        "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False,)\n",
        "  # Do not include the ImageNet classifier at the top as we don't need those classes\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)  # Apply random data augmentation from the layer we created above\n",
        "\n",
        "# Pre-trained Xception weights requires that input be scaled\n",
        "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
        "# outputs: `(inputs * scale) + offset`\n",
        "# we create a rescaling layer and apply it to \"x\" which already has data augmentation applied to it\n",
        "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
        "x = scale_layer(x)\n",
        "\n",
        "# The base model contains batchnorm layers. We want to keep them in inference mode (moving forward and not changing any weights/biases yet, only giving result)\n",
        "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
        "# base_model is running in inference mode here, hence training is false\n",
        "x = base_model(x, training=False)\n",
        "# using pooling layer\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csO-HZ2Ea3dY"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(0.001),\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "epochs =3\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El5CNMfPa-nB"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
        "# since we passed `training=False` when calling it. This means that\n",
        "# the batchnorm layers will not update their batch statistics.\n",
        "# This prevents the batchnorm layers from undoing all the training\n",
        "# we've done so far.\n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-4),  # Low learning rate\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "epochs = 5\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtMRm0R1GWv6"
      },
      "source": [
        "### 1.2 Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JKFpn4GeXpn"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/gdrive/MyDrive/Image_Classification/model_xception.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdDPa9q4GjIQ"
      },
      "source": [
        "### 1.3 Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZw3XJPhxGr"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print('Test accuracy: ',test_acc)\n",
        "print('Test loss: ',test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrm-yY_jz04Z"
      },
      "outputs": [],
      "source": [
        "# using the xception model to display images on unseen data with predicted labels - very inaccuarate, as expected.\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = load_model('/content/gdrive/MyDrive/Image_Classification/model_xception.h5')\n",
        "\n",
        "# Set the path to the folder containing the images\n",
        "folder_path = '/content/Instagram_Dataset_1/unseen'\n",
        "\n",
        "# Loop through the images in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "\n",
        "    # Load the image\n",
        "    img = image.load_img(os.path.join(folder_path, filename), target_size=(180, 180))\n",
        "\n",
        "    # Convert the image to a numpy array\n",
        "    img_array = image.img_to_array(img)\n",
        "\n",
        "    # Reshape the array to match the input shape of the VGG16 model\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Preprocess the input image (normalize pixel values to be between -1 and 1)\n",
        "    img_array = preprocess_input(img_array)\n",
        "\n",
        "    # Make a prediction on the image\n",
        "    preds = model.predict(img_array)\n",
        "    print(preds)\n",
        "\n",
        "    # Decode the prediction into a human-readable label\n",
        "    label = int(preds.argmax(axis=-1))\n",
        "    label_name={0:'beauty',1:'food',2:'memes',3:'pets',4:'travel'}\n",
        "\n",
        "\n",
        "    # Display the image with predicted label\n",
        "    plt.imshow(img)\n",
        "    plt.title(label_name[label])\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYahBJsxD-9B"
      },
      "source": [
        "#2. Transfer Learning Model - MobileNetV2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiJ-5aVIGopV"
      },
      "source": [
        "### 2.1 Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVMbMjGqGrbi"
      },
      "source": [
        "In this approach, we again use a base model, MobileNetV2 and proceed as in the first approach. Data loading and preprocessing is similar to the second approach with some small changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2jQ8zaA65Bd"
      },
      "outputs": [],
      "source": [
        "#loading essential libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvzuzMpq68l_"
      },
      "outputs": [],
      "source": [
        "#setting important parameters and loading the three required datasets\n",
        "batch_size = 64\n",
        "img_height = 160\n",
        "img_width = 160\n",
        "data_dir= \"/content/Instagram_Dataset_1/classes\"\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.1,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.1,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsZOjBdE7F6P"
      },
      "outputs": [],
      "source": [
        "#creating test_ds and loading images\n",
        "batch_size = 32\n",
        "img_height = 160\n",
        "img_width = 160\n",
        "test_dir= \"/content/Instagram_Dataset_1/test\"\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  test_dir,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRZstZe37NcA"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK6EW2xA7QLS"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "  tf.keras.layers.RandomContrast(factor=0.3),\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivWCXFCm7ebr"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_MpZ_VI7gy1"
      },
      "outputs": [],
      "source": [
        "rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZVUbtiq7hXK"
      },
      "outputs": [],
      "source": [
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "IMG_SIZE = (160, 160)\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVMErHI87hh2"
      },
      "outputs": [],
      "source": [
        "image_batch, label_batch = next(iter(train_ds))\n",
        "feature_batch = base_model(image_batch)\n",
        "print(feature_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFaz4jGA81MH"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxVlIIvK85WH"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the base model architecture\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLnI3A4h9IBZ"
      },
      "outputs": [],
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "print(feature_batch_average.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ze4i8019Qmm"
      },
      "outputs": [],
      "source": [
        "prediction_layer = tf.keras.layers.Dense(1)\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "print(prediction_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-Xi9eOK9jY5"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST0A37h195M8"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3GOZ8K49kfS"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpOw7_9q9_8k"
      },
      "outputs": [],
      "source": [
        "len(model.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivT2B-4--L0h"
      },
      "outputs": [],
      "source": [
        "initial_epochs = 5\n",
        "\n",
        "loss0, accuracy0 = model.evaluate(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ82uDQl-uDt"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds,\n",
        "                    epochs=initial_epochs,\n",
        "                    validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YLBJkpY_fa3"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJORjANj_jjk"
      },
      "outputs": [],
      "source": [
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4rThbuG_mB_"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrHHTl9V_tGq"
      },
      "outputs": [],
      "source": [
        "fine_tune_epochs = 5\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(train_ds,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=history.epoch[-1],\n",
        "                         validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IeMk5nJGxsE"
      },
      "source": [
        "### 2.2 Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFUjEbmsG1bU"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/gdrive/MyDrive/Image_Classification/model_mobilenetv2.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJFkFOGMG6Ln"
      },
      "source": [
        "### 2.3 Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxLxi6EU_1Nn"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print('Test accuracy :', accuracy)\n",
        "print ('Test loss: ', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQLI9qV5PRoo"
      },
      "source": [
        "#3. Transfer Learning Model - VGG16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBZcfiKUHyQ8"
      },
      "source": [
        "### 3.1 Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohq026XmUz5s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmKBzqUEU2P8"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_generator = ImageDataGenerator(rotation_range=90,\n",
        "                                     brightness_range=[0.1, 0.7],\n",
        "                                     width_shift_range=0.5,\n",
        "                                     height_shift_range=0.5,\n",
        "                                     horizontal_flip=True,\n",
        "                                     vertical_flip=True,\n",
        "                                     validation_split=0.15,\n",
        "                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n",
        "\n",
        "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavgbU4mU-wq"
      },
      "outputs": [],
      "source": [
        "download_dir = Path('/content/Instagram_Dataset_1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XOmZmv-VLvg"
      },
      "outputs": [],
      "source": [
        "train_data_dir = download_dir/'classes'\n",
        "test_data_dir = download_dir/'test'\n",
        "\n",
        "traingen = train_generator.flow_from_directory(train_data_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode='categorical',\n",
        "                                               subset='training',\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True,\n",
        "                                               seed=42)\n",
        "\n",
        "validgen = train_generator.flow_from_directory(train_data_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode='categorical',\n",
        "                                               subset='validation',\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True,\n",
        "                                               seed=42)\n",
        "\n",
        "testgen = test_generator.flow_from_directory(test_data_dir,\n",
        "                                             target_size=(224, 224),\n",
        "                                             class_mode=None,\n",
        "                                             batch_size=1,\n",
        "                                             shuffle=False,\n",
        "                                             seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQZjiZSbVbKt"
      },
      "outputs": [],
      "source": [
        "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
        "    \"\"\"\n",
        "    Compiles a model integrated with VGG16 pretrained layers\n",
        "\n",
        "    input_shape: tuple - the shape of input images (width, height, channels)\n",
        "    n_classes: int - number of classes for the output layer\n",
        "    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n",
        "    fine_tune: int - The number of pre-trained layers to unfreeze.\n",
        "                If set to 0, all pretrained layers will freeze during training\n",
        "    \"\"\"\n",
        "\n",
        "    # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
        "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
        "    conv_base = VGG16(include_top=False,\n",
        "                     weights='imagenet',\n",
        "                     input_shape=input_shape)\n",
        "\n",
        "    # Defines how many layers to freeze during training.\n",
        "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
        "    # depending on the size of the fine-tuning parameter.\n",
        "    if fine_tune > 0:\n",
        "        for layer in conv_base.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        for layer in conv_base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
        "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
        "    top_model = conv_base.output\n",
        "    top_model = Flatten(name=\"flatten\")(top_model)\n",
        "    top_model = Dense(4096, activation='relu')(top_model)\n",
        "    top_model = Dense(1072, activation='relu')(top_model)\n",
        "    top_model = Dropout(0.2)(top_model)\n",
        "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
        "\n",
        "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
        "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
        "\n",
        "    # Compiles the model for training.\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwTouT-SWser"
      },
      "outputs": [],
      "source": [
        "input_shape = (224, 224, 3)\n",
        "optim_1 = Adam(learning_rate=0.001)\n",
        "n_classes=5\n",
        "\n",
        "n_steps = traingen.samples // BATCH_SIZE\n",
        "n_val_steps = validgen.samples // BATCH_SIZE\n",
        "n_epochs = 6\n",
        "\n",
        "# First we'll train the model without Fine-tuning\n",
        "vgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPkawgtlXJDL"
      },
      "outputs": [],
      "source": [
        "!pip install livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UQPXY2nW6ot"
      },
      "outputs": [],
      "source": [
        "from livelossplot.inputs.keras import PlotLossesCallback\n",
        "\n",
        "plot_loss_1 = PlotLossesCallback()\n",
        "\n",
        "# ModelCheckpoint callback - save best weights\n",
        "tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.best.hdf5',\n",
        "                                  save_best_only=True,\n",
        "                                  verbose=1)\n",
        "\n",
        "# EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss',\n",
        "                           patience=2,\n",
        "                           restore_best_weights=True,\n",
        "                           mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5N8BOehXPJQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "vgg_history = vgg_model.fit(traingen,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            epochs=n_epochs,\n",
        "                            validation_data=validgen,\n",
        "                            steps_per_epoch=n_steps,\n",
        "                            validation_steps=n_val_steps,\n",
        "                            callbacks=[tl_checkpoint_1, early_stop, plot_loss_1],\n",
        "                            verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3zAqSDgXXtJ"
      },
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "vgg_model.load_weights('tl_model_v1.weights.best.hdf5') # initialize the best trained weights\n",
        "\n",
        "true_classes = testgen.classes\n",
        "class_indices = traingen.class_indices\n",
        "class_indices = dict((v,k) for k,v in class_indices.items())\n",
        "\n",
        "vgg_preds = vgg_model.predict(testgen)\n",
        "vgg_pred_classes = np.argmax(vgg_preds, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obm4x1QlXaMl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vgg_acc = accuracy_score(true_classes, vgg_pred_classes)\n",
        "print(\"VGG16 Model Accuracy without Fine-Tuning: {:.2f}%\".format(vgg_acc * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFEPmQu9XePs"
      },
      "outputs": [],
      "source": [
        "# trying fine tuning\n",
        "# Reset our image data generators\n",
        "traingen.reset()\n",
        "validgen.reset()\n",
        "testgen.reset()\n",
        "\n",
        "# Use a smaller learning rate\n",
        "optim_2 = Adam(lr=0.0001)\n",
        "\n",
        "# Re-compile the model, this time leaving the last 2 layers unfrozen for Fine-Tuning\n",
        "vgg_model_ft= create_model(input_shape, n_classes, optim_2, fine_tune=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwqGDkSTXjLz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "plot_loss_2 = PlotLossesCallback()\n",
        "\n",
        "# Retrain model with fine-tuning\n",
        "vgg_ft_history = vgg_model_ft.fit(traingen,\n",
        "                                  batch_size=BATCH_SIZE,\n",
        "                                  epochs=n_epochs,\n",
        "                                  validation_data=validgen,\n",
        "                                  steps_per_epoch=n_steps,\n",
        "                                  validation_steps=n_val_steps,\n",
        "                                  callbacks=[tl_checkpoint_1, early_stop, plot_loss_2],\n",
        "                                  verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPpkDcY-H8Wr"
      },
      "source": [
        "### 3.2 Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdnDnAPuH-jt"
      },
      "outputs": [],
      "source": [
        "vgg_model.save(\"/content/gdrive/MyDrive/Image_Classification/model_VGG16.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qvEru8pIKaE"
      },
      "source": [
        "### 3.3 Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eURmxm32XoRi"
      },
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "vgg_model_ft.load_weights('tl_model_v1.weights.best.hdf5') # initialize the best trained weights\n",
        "\n",
        "vgg_preds_ft = vgg_model_ft.predict(testgen)\n",
        "vgg_pred_classes_ft = np.argmax(vgg_preds_ft, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PumuHJC7XqOD"
      },
      "outputs": [],
      "source": [
        "vgg_acc_ft = accuracy_score(true_classes, vgg_pred_classes_ft)\n",
        "print(\"VGG16 Model Accuracy with Fine-Tuning: {:.2f}%\".format(vgg_acc_ft * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSiTGyomxbPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}